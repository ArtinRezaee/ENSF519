{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Term Test 2 - Nov 27, 2019 </center></h2>\n",
    "<h2> <center> 100 marks - 2 hours </center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Your Full Name:` Artin Rezaee-Anzabee\n",
    "\n",
    "`Your Student ID:` 10121269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1: Classification (35 pts)\n",
    "\n",
    "In this question, you will apply some classification methods on the Breast Cancer dataset, provided by SKLearn. The breast cancer dataset is a classic binary classification dataset that can be found here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "     \n",
    "     \n",
    "\n",
    "After loading the dataset, spilt the data into test and train subsets using K-Fold(k=5) - use random_state=0. Then follow the 3 steps, explianed below:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that has not been mentioned.\n",
    "</div>\n",
    "\n",
    "* Step1:  Build 3 classification models using LinearSVC, GaussianNB, and KNeighborsClassifier from SKLearn, with default settings. Each model gets all features of the dataset as the feature set and predicts the the target as 'malignant' or 'benign'. \n",
    "\n",
    "    * Report your prediction results as three confusion matrices; one matrix per model (representing the mean of 5 folds results per model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSV:  [[24  2]\n",
      " [ 7 80]]\n",
      "GNB:  [[24  2]\n",
      " [ 3 84]]\n",
      "KNC: [[25  1]\n",
      " [ 6 81]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "kfold = KFold(n_splits=5, random_state=0)\n",
    "# for train_index, test_index in kfold.split(X):\n",
    "#     print(\"Train Index: \", train_index, \"\\n\")\n",
    "#     print(\"Test Index: \", test_index)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "lsv = LinearSVC().fit(X_train, y_train)\n",
    "lsv_pred = lsv.predict(X_test)\n",
    "lsv_conf = confusion_matrix(y_test, lsv_pred)\n",
    "print('LSV: ', lsv_conf)\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "gnb_pred = gnb.predict(X_test)\n",
    "gnb_conf = confusion_matrix(y_test, gnb_pred)\n",
    "print('GNB: ', gnb_conf)\n",
    "                      \n",
    "knc = KNeighborsClassifier().fit(X_train, y_train)\n",
    "knc_pred = knc.predict(X_test)\n",
    "knc_conf = confusion_matrix(y_test, knc_pred)\n",
    "print('KNC:', knc_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which model(s) has the highest False Postive and which one(s) has the highest False Negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSV and GNV have the highest FP and LSV has the highest false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2: Normalize your data using StandardScaler. Then build the same classification models (LinearSVC, GaussianNB, and KNN) this time using the scaled data. \n",
    "\n",
    "    * Report your prediction results in the form of three confusion matrices, again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSV:  [[25  1]\n",
      " [ 1 86]]\n",
      "GNB:  [[24  2]\n",
      " [ 7 80]]\n",
      "KNC: [[26  0]\n",
      " [ 4 83]]\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lsv = LinearSVC().fit(X_train_scaled, y_train)\n",
    "lsv_pred = lsv.predict(X_test_scaled)\n",
    "lsv_conf = confusion_matrix(y_test, lsv_pred)\n",
    "print('LSV: ', lsv_conf)\n",
    "\n",
    "gnb = GaussianNB().fit(X_train_scaled, y_train)\n",
    "gnb_pred = gnb.predict(X_test_scaled)\n",
    "gnb_conf = confusion_matrix(y_test, gnb_pred)\n",
    "print('GNB: ', gnb_conf)\n",
    "                      \n",
    "knc = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "knc_pred = knc.predict(X_test_scaled)\n",
    "knc_conf = confusion_matrix(y_test, knc_pred)\n",
    "print('KNC:', knc_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Identify which model(s) has most improved with scaling. Explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSV has imporoved the most since it has less FP and FNs. Scaling the data seems to be allowing the model to fit a better decision boundry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: Combine the above three models using hard vote in VotingClassifier. Use the same Scaled (normalized) data as step 2. \n",
    "\n",
    "    * Again, report your prediction result in the form of confusion matrix (this time, one matrix). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  1],\n",
       "       [ 2, 85]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lsv = LinearSVC().fit(X_train_scaled, y_train)\n",
    "gnb = GaussianNB().fit(X_train_scaled, y_train)\n",
    "knc = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "eAlg = VotingClassifier(estimators=[('LSV',lsv),('GNB',gnb),('KNC',knc)],\n",
    "                        voting='hard').fit(X_train_scaled, y_train)\n",
    "\n",
    "confusion_matrix(y_test, eAlg.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is there any improvement using the combined model compared to step2 results? If so, explain where you see improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined model performs slightly worse than the LSV alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Regression (40 pts)\n",
    "\n",
    "In this question, you're asked to build a regression model to predict the price of a new house based on the data of previously sold houses that is provided in a dataset from California Housing Prices.\n",
    "\n",
    "#### Part A. Random Forest Regression  \n",
    "As a first step, train a simple RandomForestRegressor model by following these steps:\n",
    "\n",
    "* Load the data from here: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n",
    "* Split it into train (2/3) and test (1/3), with random_state=42. \n",
    "\n",
    "* Scale all the features (except label) to map them into [0,1] interval. \n",
    "* Use the scaled training set to build a basic RandomForestRegressor model with n_estimators=100 and random_state=0.\n",
    "* Report the score (R^2) of your model on train and test set. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that has not been mentioned.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.7501497057930221\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=0.67, test_size=0.33, random_state=0)\n",
    "\n",
    "X_train_scaled = scale(X_train)\n",
    "X_test_scaled = scale(X_test)\n",
    "regr = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "print('score: ', regr.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. Improve the model\n",
    "\n",
    "Now try to improve the model by tuning the parameters:\n",
    "\n",
    "* Use GridSearchCV (with 5-fold cross-validation to make the train and validation sets) to tune part A's RandomForestRegressor's parameters as follows: \n",
    "    * max_features with 'auto', 'sqrt', and 'log2' \n",
    "    * max_depth with None, 10, 20, and 30\n",
    "    * min_samples_split with 2, 5, and 10\n",
    "\n",
    "* Report the tuned values of parameters as well as the score of the tuned model, on both train and test sets.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "Use the same scaled data and the same train, test split as Part A. Also for any unmentioned paramter, use the same default as Part A.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_split': 5}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6812, 13828]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f189f6497938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train set score: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test set score: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    426\u001b[0m                              % self.best_estimator_)\n\u001b[1;32m    427\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimetric_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# XXX: Remove the check in 0.23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'continuous-multioutput'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             warnings.warn(\"The default value of multioutput (not exposed in \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6812, 13828]"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth': [10, 20, 30],\n",
    "              'min_samples_split': [2, 5, 10]\n",
    "             }\n",
    "grid_search = GridSearchCV(regr, param_grid, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_split': 5}\n",
      "Train set score: 0.96\n",
      "Test set score: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Train set score: {:.2f}\".format(grid_search.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C. Dimension Reduction\n",
    "\n",
    "Reduce the dimensionality of the data set to 4 dimensions, using PCA and RFE and repeat the predictions and report the scores. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "Make sure you use the same set up as Part B. That is the same train, test split, the same scaled data, and the same tuned values (i.e., you just take the one best tuned model and no need to do the corss-validation tuning again). \n",
    "</div>\n",
    "\n",
    "__PCA__:\n",
    "\n",
    "* Use PCA to reduce the features dimensions to 4 components. Note that knowing where and when PCA must be applied, is part of the question.\n",
    "* Now use the new feature set (4 PCA components rather than the original features) and report the R^2 score on the test set, using the same tuned  RandomForestRegressor model as part B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.5592143925713742\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "best_estimator = grid_search.best_estimator_;\n",
    "\n",
    "pca = PCA(n_components = 4).fit(X_train_scaled)\n",
    "gs_pca = best_estimator.fit(pca.transform(X_train_scaled), y_train)\n",
    "score = gs_pca.score(pca.transform(X_test_scaled), y_test)\n",
    "print('score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RFE__:\n",
    "\n",
    "* Use RFE with a simple linear regression model (using default values) to reduce the data to its 4 most informative features. Note that knowing how to properly apply RFE, is part of the question.\n",
    "* Now use the new feature set (top 4 features rather than the original features) and report the R^2 score on the test set, using the same tuned  RandomForestRegressor model as part B. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.7937082800882981\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "rfe_lreg = RFE(LinearRegression(), n_features_to_select=4).fit(X_train_scaled, y_train)\n",
    "gs_rfe = best_estimator.fit(rfe_lreg.transform(X_train_scaled), y_train)\n",
    "score = gs_rfe.score(rfe_lreg.transform(X_test_scaled), y_test)\n",
    "print('score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptually, what is the key difference between dimension reduction using RFE and PCA. \n",
    "* Explian your observation on the regresion results of RFE and PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA grabs features that have the most variance and ignores the rest while RFE recursively removes features, builds a model using the remaining attributes and calculates model accuracy. As the observation suggests, our accuracy is much better using RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Text Analysis (25 pts)\n",
    "\n",
    "In this question, you are given a dataset of movie_plots and asked to build an LDA (Latent Dirichlet Allocation) model to extract different topics that exist among these plots, by following these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Read the given \"movie_plots.csv\" file and load it to a pandas dataframe.\n",
    "- Vectorize the plots (provided in the \"Plot\" feature) using CountVectorizer, while considering the followings:\n",
    "    - use only bi-grams tokens. \n",
    "    - ignore english_stop_words.\n",
    "    - keep the maximum number of features at 10,000.\n",
    "    - drop features that appear in more than 5% of movies.\n",
    "    - drop features that appear in less than 10 movies.\n",
    "- Next build an LDA model (random_state=0) with 10 topics and fit it with all the vectorized movie plots.   \n",
    "- Now print the topics, using the 5 most important words per topics\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that have not been mentioned.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top in 1: ['drive away', 'drug dealer', 'secur guard', 'park lot', 'polic station']\n",
      "Top in 2: ['san francisco', 'real world', 'make way', 'attempt kill', 'las vega']\n",
      "Top in 3: ['white hous', 'secret servic', 'servic agent', 'hotel room', 'fbi agent']\n",
      "Top in 4: ['world war', 'new orlean', 'agre help', 'shot kill', 'say goodby']\n",
      "Top in 5: ['young man', 'follow day', 'north pole', 'make way', 'heart attack']\n",
      "Top in 6: ['las vega', 'heart attack', 'footbal team', 'old friend', 'true love']\n",
      "Top in 7: ['final scene', 'young woman', 'day later', 'way home', 'movi end']\n",
      "Top in 8: ['manag escap', 'air forc', 'tri kill', 'make way', 'fbi agent']\n",
      "Top in 9: ['spend night', 'young woman', 'month later', 'car crash', 'credit card']\n",
      "Top in 10: ['arriv home', 'tell stori', 'gas station', 'young man', 'christma tree']\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "data = pd.read_csv('movie_plots.csv')\n",
    "plot = data['Plot']\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\", min_df=10, max_df=0.05, max_features=10000)\n",
    "count_vectorizer.fit(plot)\n",
    "vect = count_vectorizer.transform(plot)\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0).fit(vect)\n",
    "\n",
    "for i, topic in enumerate(lda.components_, start=1):\n",
    "    top5_indecies = topic.argsort()[::-1][:5]\n",
    "    features = count_vectorizer.get_feature_names()\n",
    "    print('Top in '+str(i)+':', [features[j] for j in top5_indecies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now add uni-gram and tri-gram tokens to the bi-gram tokens, and keep everything else the same and redo the LDA modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top in 1: ['mari', 'georg', 'ray', 'johnni', 'laura']\n",
      "Top in 2: ['charli', 'jimmi', 'henri', 'anni', 'edward']\n",
      "Top in 3: ['harri', 'danni', 'kevin', 'andi', 'alien']\n",
      "Top in 4: ['max', 'jess', 'marcus', 'ryan', 'roger']\n",
      "Top in 5: ['adam', 'scott', 'maggi', 'martin', 'kyle']\n",
      "Top in 6: ['anna', 'bobbi', 'tim', 'luke', 'clair']\n",
      "Top in 7: ['mike', 'sam', 'kate', 'chris', 'eddi']\n",
      "Top in 8: ['joe', 'alex', 'jane', 'billi', 'band']\n",
      "Top in 9: ['ben', 'nick', 'jake', 'matt', 'luci']\n",
      "Top in 10: ['ami', 'tommi', 'rose', 'helen', 'ted']\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\", min_df=10, max_df=0.05, max_features=10000)\n",
    "count_vectorizer.fit(plot)\n",
    "vect = count_vectorizer.transform(plot)\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0).fit(vect)\n",
    "\n",
    "for i, topic in enumerate(lda.components_, start=1):\n",
    "    top5_indecies = topic.argsort()[::-1][:5]\n",
    "    features = count_vectorizer.get_feature_names()\n",
    "    print('Top in '+str(i)+':', [features[j] for j in top5_indecies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - What major changes you see in the resulting topics?  Explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words in the first section are multi part words where as in the second part they are single words. It is also interesting that\n",
    "in the second section all top words are names. Here by using more ns, we are using more features and more specific features. Thus, we get more specif words as it is seen in the second run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
